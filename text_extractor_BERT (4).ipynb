{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiCFGRO_HmcY",
        "outputId": "b3212d1b-bca7-4698-a0ce-3459feb7a532"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: keybert in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.16.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.12/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from keybert) (5.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.4.0->keybert) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.4.0->keybert) (2.19.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->keybert) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m125.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers spacy keybert PyPDF2\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHj4gnl8IJyl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import spacy\n",
        "from keybert import KeyBERT\n",
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZQvevGitDny",
        "outputId": "0e7dea6e-8a52-4e60-dc0c-92e132896810"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Running on GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "gpu_status = \"GPU: \" + torch.cuda.get_device_name(0) if device == 0 else \"CPU\"\n",
        "print(f\" Running on {gpu_status}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "kw_model = KeyBERT()"
      ],
      "metadata": {
        "id": "crCkwtQkkTN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tiu2K2U2kkTi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goilOfXkqw3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "950fc4a8-fc6d-4896-898d-5f2d838d4852"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/multitask_fda_extractor.zip\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/special_tokens_map.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/pytorch_model.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/tokenizer_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/labels.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/tokenizer.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/multitask_fda_extractor.zip -d /content/drive/MyDrive/multitask_fda_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syFggDQHq0P-",
        "outputId": "b46b4bc2-5cdd-4502-e5b1-e00e702e2253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/ and are newly initialized: ['bert.embeddings.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "MODEL_PATH = \"/content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=6)  # 6 heads\n",
        "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8HCmMtvJWRI"
      },
      "outputs": [],
      "source": [
        "HEADS = [\"Hardware\", \"Software Components\", \"AI Models\", \"Data Pipelines\", \"User Interface\", \"Integration\"]\n",
        "# Helper: run multitask predictions\n",
        "def predict_heads(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits.squeeze()\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()\n",
        "    return {HEADS[i]: float(probs[i]) for i in range(len(HEADS))}\n",
        "\n",
        "# Helper: extract keywords from text\n",
        "def extract_keywords(text, top_n=5):\n",
        "    doc = nlp(text)\n",
        "    chunks = list(set(chunk.text.lower() for chunk in doc.noun_chunks if len(chunk.text) > 3))\n",
        "    if chunks:\n",
        "        return chunks[:top_n]\n",
        "    keybert_kw = kw_model.extract_keywords(text, top_n=top_n)\n",
        "    return [kw for kw, _ in keybert_kw]\n",
        "\n",
        "# Split PDF into text chunks\n",
        "def extract_chunks_from_pdf(pdf_path, chunk_size=300):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    full_text = \" \".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
        "    words = full_text.split()\n",
        "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    print(f\"Extracted {len(chunks)} chunks from PDF: {chunks}\")\n",
        "    return chunks\n",
        "\n",
        "# Main pipeline: classify chunks + extract keywords\n",
        "def analyze_text_chunks(chunks, threshold=0.5):\n",
        "    results = {head: [] for head in HEADS}\n",
        "    for chunk in chunks:\n",
        "        head_probs = predict_heads(chunk)\n",
        "        for head, prob in head_probs.items():\n",
        "            if prob >= threshold:\n",
        "                kws = extract_keywords(chunk)\n",
        "                results[head].extend(kws)\n",
        "    for head in results:\n",
        "        results[head] = list(set(results[head]))\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "PDF_PATH = list(uploaded.keys())[0]\n",
        "chunks = extract_chunks_from_pdf(PDF_PATH)\n",
        "output = analyze_text_chunks(chunks)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "Wyklux6Y-mTw",
        "outputId": "28c831bb-b031-45db-bc9c-025c23dfe3ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a34a31c2-0615-48b4-83a6-fdba4c9f39d9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a34a31c2-0615-48b4-83a6-fdba4c9f39d9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving K250236.pdf to K250236 (4).pdf\n",
            "Extracted 11 chunks from PDF: ['U.S. Food & Drug Administration 10903 New Hampshire Avenue D o c I D # 0 4 0 1 7 . 0 7.05 Silver Spring, MD 20993 www.fda.gov May 30, 2025 Hyperfine, Inc. Christine Kupchick Sr. Manager, Global Regulatory 351 New Whitfield St Guilford, Connecticut 06437 Re: K250236 Trade/Device Name: Swoop® Portable MR Imaging® System (V2) Regulation Number: 21 CFR 892.1000 Regulation Name: Magnetic Resonance Diagnostic Device Regulatory Class: Class II Product Code: LNH, MOS Dated: May 7, 2025 Received: May 7, 2025 Dear Christine Kupchick: We have reviewed your section 510(k) premarket notification of intent to market the device referenced above and have determined the device is substantially equivalent (for the indications for use stated in the enclosure) to legally marketed predicate devic es marketed in interstate commerce prior to May 28, 1976, the enactment date of the Medical Device Amendments, or to devices that have been reclassified in accordance with the provisions of the Federal Food, Drug, and Cosmetic Act (the Act) that do not req uire approval of a premarket approval application (PMA). You may, therefore, market the device, subject to the general controls provisions of the Act. Although this letter refers to your product as a device, please be aware that some cleared products may i nstead be combination products. The 510(k) Premarket Notification Database available at https://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfpmn/pmn.cfm identifies combination product submissions. The general controls provisions of the Act include requirements for annual registration, listing of devices, good manufacturing practice, labeling, and prohibitions against misbranding and adulteration. Please n ote: CDRH does not evaluate information related to contract liability warranties. We remind you, however, that device labeling must be truthful and not misleading. If your device is classified (see above) into either class II (Special Controls) or class III (PMA), it may be subject to additional', 'controls. Existing major regulations affecting your device can be found in the Code of Federal Regulations, Title 21, Parts 800 to 898. In addition, FDA may publish further announcements concerning your device in the Federal Register . K250236 - Christine Kupchick Page 2 Additional information about changes that may require a new premarket notification are provided in the FDA guidance documents entitled \"Deciding When to Submit a 510(k) for a Change to an Existing Device\" (https://www.fda.gov/media/99812/download ) and \"Deciding When to Submit a 510(k) for a Software Change to an Existing Device\" ( https://www.fda.gov/media/99785/download ). Your device is also subject to, among other requirements, the Quality System (QS) regulation (21 CFR Part 820), which includes, but is not limited to, 21 CFR 820.30, Design controls; 21 CFR 820.90, Nonconforming product; and 21 CFR 820.100, Corrective and preventive action. Please note that regardless of whether a change requires premarket review, the QS regulation requires device manufacturers to review and approve changes to device design and production (21 CFR 820.30 and 21 CFR 820.70) and document chang es and approvals in the device master record (21 CFR 820.181). Please be advised that FDA\\'s issuance of a substantial equivalence determination does not mean that FDA has made a determination that your device complies with other requirements of the Act or any Federal statutes and regulations administered by other Fede ral agencies. You must comply with all the Act\\'s requirements, including, but not limited to: registration and listing (21 CFR Part 807); labeling (21 CFR Part 801); medical device reporting (reporting of medical device -related adverse events) (21 CFR Part 803) for devices or postmarketing safety reporting (21 CFR Part 4, Subpart B) for combination products (see https://www.fda.gov/combination -products/guidance -regulatory -information/postmarketing -safety -reporting - combination -products ); good manufacturing practice requirements as set forth', 'in the quality systems (QS) regulation (21 CFR Part 820) for devices or current good manufacturing practices (21 CFR Part 4, Subpart A) for combination products; and, if applicable, the electronic pr oduct radiation control provisions (Sections 531-542 of the Act); 21 CFR Parts 1000 -1050. All medical devices, including Class I and unclassified devices and combination product device constituent parts are required to be in compliance with the final Unique Device Identification System rule (\"UDI Rule\"). The UDI Rule requires, among other thing s, that a device bear a unique device identifier (UDI) on its label and package (21 CFR 801.20(a)) unless an exception or alternative applies (21 CFR 801.20(b)) and that the dates on the device label be formatted in accordance with 21 CFR 801.18. The UDI R ule (21 CFR 830.300(a) and 830.320(b)) also requires that certain information be submitted to the Global Unique Device Identification Database (GUDID) (21 CFR Part 830 Subpart E). For additional information on these requirements, please see the UDI System webpage at https://www.fda.gov/medical -devices/device -advice - comprehensive -regulatory -assistance/unique -device -identification -system -udi-system . Also, please note the regulation entitled, \"Misbranding by reference to premarket notification\" (21 CFR 807.97). For questions regarding the reporting of adverse events under the MDR regulation (21 CFR Part 803), please go to https://www.fda.gov/medical -devices/medical -device -safety/medical -device -reporting - mdr-how-report -medical -device -problems . For comprehensive regulatory information about medical devices and radiation -emitting products, including information about labeling regulations, please see Device Advice ( https://www.fda.gov/medical - devices/device -advice -comprehensive -regulatory -assistance ) and CDRH Learn (https://www.fda.gov/training -and-continuing -education/cdrh -learn ). Additionally, you may contact the Division of Industry and Consumer Education (DICE) to ask a question about a specific regulatory topic. See the DICE website ( https://www.fda.gov/medical -devices/device -advice -comprehensive -regulatory - K250236 - Christine', 'Kupchick Page 3 assistance/contact -us-division -industry -and-consumer -education -dice) for more information or contact DICE by email ( DICE@fda.hhs.gov ) or phone (1 -800-638-2041 or 301 -796-7100). Sincerely, Daniel M. Krainak , Ph.D . Assistant Director DHT8C: Division of Radiological Imaging and Radiation Therapy Devices OHT8: Office of Radiological Health Office of Product Evaluation and Quality Center for Devices and Radiological Health Enclosure 510(k) Summary Swoop® Portable MR Imaging® System (V2) K250236 510( K) SUBMITTER Company Name: Hyperfine, Inc. Company Address: 351 New Whitfield St Guilford, CT 06437 CONTACT Name: Christine Kupchick Telephone: (203) 343 -3404 Email: ckupchick@hyperfine.io Date Prepared: May 2 2, 2025 DEVICE IDENTIFICATION Trade Name: Swoop® Portable MR Imaging® System (V2) Common Name: Magnetic Resonance Imaging Regulation Number: 21 CFR 892.1000 Classification Name: System, Nuclear Magnetic Resonance Imaging Coil, Magnetic Resonance Product Code: LNH; MOS Regulatory Class: Class II PREDICATE DEVICE INFORMATION The subject Swoop Portable MR Imaging System (V2) is substantially equivalent to the predicate Swoop System (K240944). DEVICE DESCRIPTION The Swoop System (V2) is portable, ultra -low field MRI device that enables visualization of the internal structures of the head using standard magnetic resonance imaging contrasts. The main interface is a commercial off -the-shelf device that is used for operating the system, providing access to scan orders, exam setup, exam execution, viewing MRI image data for quality control purposes, and PACS interactions. The system can generate MRI data sets with a broad range of contrasts. The Swoop system user interface includes touch screen menus, controls, indicators, and navigation icons that allow the operator to control the system and to preview images. The Swoop System (V2) image reconstruction algorithm utilizes deep learning for optimized image quality. INDICATIONS FOR USE The Swoop Portable MR Imaging System (V2) is a portable, ultra -low field magnetic resonance imaging device', 'for producing images that display the internal structure of the head where full diagnostic examination is not clinically practical. When interpreted by a trained physician, these images provide information that can be useful in determining a diagnosis. SUBSTANTIAL EQUIVALENCE DISCUSSION The table below compares the subject device to the predicate. Specification Subject Swoop Portable MR Imaging System (V2) Predicate Swoop Portable MR Imaging System (V1) (K240944) INTENDED USE Intended Use/ Indications for Use The Swoop Portable MR Imaging System is a portable, ultra -low field magnetic resonance imaging device for producing images that display the internal structure of the head where full diagnostic examination is not clinically practical. When interpreted by a trained physician, these images provide information that can be useful in determining a diagnosis. Same Patient Population Adult and pediatric patients (≥ 0 years) Same Anatomical Sites Head Same Environment of Use At the point of care in professional health care facilities such as emergency rooms, intensive/critical care units, hospitals, outpatient, or rehabilitation centers. Same Energy Used and/or delivered Magnetic Resonance Same MAGNET Field strength 64.9 mT (nominal) 63.3 ± 2.0 mT Type Permanent magnet Same Patient accessible bore size 36.0 in. width, 13.4 in. height 24.0 in. width, 12.4 in. height Magnet weight 712 lbs. 705 lbs. GRADIENT SYSTEM Maximum gradient amplitude X: 33.9 mT/m Y: 33.2 mT/m Z: 66.2 mT/m X: 24.3 mT/m Y: 22.9 mT/m Z: 38.5 mT/m Rise time (zero to max) X: 1.8 ms Y: 1.8 ms Z: 5.1 ms X: 2.1 ms Y: 2.0 ms Z: 3.8 ms Slew rate X: 18.8 T/m/s Y: 18.4 T/m/s Z: 13.0 T/m/s X: 24 T/m/s Y: 22 T/m/s Z: 21 T/m/s RF COIL Type Transmit/receive Same Transmit coil design Linear Same OTHER Patient weight capacity 1.6kg -200 kg Same Operation temperature 15-30 C Same Warm', 'up time <3 minutes Same Temperature control No Same Humidity control No Same SEQUENCES & IMAGE PROCESSING T1W sequences • T1 (Standard), T1 (Gray/White) • Advanced Gridding reconstruction • Same • Same T2W sequences • T2, T2 (Fast) • Advanced Gridding reconstruction • Same • Same FLAIR sequences • FLAIR, FLAIR (Fast) • Advanced Gridding reconstruction • FLAIR only • Same DWI/ADC sequences • DWI/ADC, DWI/ADC (Fast) • Advanced Gridding + FISTA • DWI/ADC only • FISTA only Image post -processing (All Sequences) • Advanced Denoising • Image orientation transform • Geometric distortion correction • Receive coil intensity correction • Advanced Interpolation • DICOM output • Same • Same • Same • Same • n/a • Same The subject device and the predicate device have the same intended use, operating principles, and similar technological characteristics. The subject device differs from the predicate in hardware and software. These differences do not raise new questions of safety and efficacy as compared to the predicate. NON-CLINICAL PERFORMANCE The subject device passed all the testing in accordance with internal requirements and applicable standards to support substantial equivalence to the predicate. Test Test Description Applicable Standard(s) Verification Verification testing in accordance with the design requirements. • IEC 62304:2015 • FDA Guidance, “Content of Premarket Submissions for Device Software Functions” • NEMA MS 1 -2008 (R2020) • NEMA MS 3 -2008 (R2020) • NEMA MS 9 -2008 (R2020) • NEMA MS 12 -2016 • NEMA MS 8 -2016 • American College of Radiology (ACR) Phantom Test Guidance for Use of the Large MRI Phantom for the ACR MRI Accreditation Program • American College of Radiology standards for named sequences • FDA Guidance, “Cybersecurity in Medical Devices: Quality System Considerations and Content of Premarket Submissions” Validation Validation to ensure the subject device meets user needs and performs', 'as intended. • FDA Guidance, “Content of Premarket Submissions for Device Software Functions” • FDA Guidance, “Applying Human Factors and Usability Engineering to Medical Devices • IEC 62366 -1:2015+AMD1:2020 Biocompatibility Biocompatibility evaluation of patient - contacting materials. • ISO 10993 -1:2018 • ISO 10993 -5:2009 • ISO 10993 -10:2021 • ISO 10993 -23:2021 Reprocessing Cleaning and disinfection evaluation of patient - contacting materials. • FDA Guidance, “Reprocessing Medical Devices in Health Care Settings: Validation Methods and Labeling” • ISO 17664:2021 • ASTM F3208 -20 • AAMI TIR12:2020 • ANSI/AAMI ST98:2022 Test Test Description Applicable Standard(s) Safety Electrical Safety, EMC, and Essential Performance testing. • IEC 60601 -1:2005 Ed.3+A1; A2 • IEC 60601 -1-2:2014 Ed.4+A1 • IEC 60601 -1-6:2010 Ed.3+A1; A2 • IEC 60601 -2-33:2015 Ed. 3.2 ADVANCED RECONSTRUCTION PERFORMANCE ANALYSIS AND VALIDATION Performance analysis and validation of the subject device Advanced Reconstruction models was performed using a test dataset entirely independent from the dataset used for model training. The test dataset comprise s a total of 58 individual subjects and 313 unique images collected using sequence types available on the subject device. For each subject, a subset of the following sequences , chosen appropriately for the indication for imaging, was scanned: T1 Gray -White, T1 Standard, T2, T2 Fast, FLAIR, FLAIR Fast, DWI , DWI Fast (DWI b=0, b=900, ADC). Axial, Sagittal, and Coronal orientations were included; for DWI sequences , only Axial was available. A description of the acceptance criteria and subset of data used for each test is included in the test summaries below. In all cases, models are trained and validated with MRI data and images as the only inputs and outputs; there are no confounding factors and clinical subgroups are not defined or considered. While gender and age are available for most subjects, age, gender ,', 'ethnic background, and pathology are not expected to influence model architecture. Performance Analysis: Study Design: Advanced Reconstruction was assessed for robustness, stability, and generalizability over a variety of subjects, design parameters, artifacts, and scan conditions using reference -based metrics. A set of images including Swoop data, high field images, and synthetic contrast images, was used as ground truth target images. Test input data (synthetic k -space generated from the target images) was reconstructed using both Advanced and Linear Reconstruc tion, and the similarity to the original ground truth image was compared between the two reconstruction methods. Reconstruction outputs with motion and zipper artifacts were qualitatively assessed. Reference Standard and Metrics: Normalized mean squared error (NMSE) and structural similarity index (SSIM) were used to compare the ability of Advanced Reconstruction to reproduce the ground truth image compared to Linear Reconstruction. Dataset and Sample Size per Model: Model/Sequence group : T1, T2, FLAIR Patients 40 Images 111 Demographics Gender Female 38% Male 55% Unknown 7% Age 18-35 5% 35-60 28% 60+ 62% 2+* 5% Ethnicity Data Not Recorded Number of sites 4 Equipment Type Swoop v2 † Pathology Atrophy, Cerebellar Infarct, Chronic Infarct, Demyelinating diseases, Embolic Infarct, Hydrocephalus , Infarct, Intracerebral Hemorrhage, Intraparenchymal Hemorrhage, Lesion, M1 Occlusion, Mass Effect, Seizure, Stroke, Subdural Hemorrhage, Traumatic Brain Injury, Tumor, White Matter Disease, White Matter Lesion * Partially anonymized † Contained Swoop Mk1.9 (<1%) Model/Sequence group : DWI # of patient 29 # Images 94 Demographics Gender Female 35% Male 55% Unknown 10% Age 18-35 7% 35-60 21% 60+ 65% 2+* 7% Ethnicity Data Not Recorded Number of sites 4 Equipment Type Swoop v 2 Pathology Atrophy, Cerebella Infarct, Chronic Infarct, Demyelinating Diseases, Embolic Infarct, Infarct, Intracerebral Hemorrhage, Intraparenchymal Hemorrhage, Lesion, M1 Occlusion, Mass Effect, Stroke, Subdural Hemorrhage, Traumatic Brain Injury, Tumor, White Matter Disease', '* Partially anonymized Study Results: For all models and all test datasets NMSE was reduced and SSIM was improved for Advanced Reconstruction test images compared to Linear Reconstruction test images. Advanced Reconstruction preserved the presentation of motion and zipper artifacts , and no unexpected output was observed. Contrast -to-Noise Ratio Validation Study Design: Regions of interest (ROI) encompassing pathologies were annotated and reviewed by two American Board of Radiology (ABR) certified radiologists. The contrast -to-noise of hyper - and hypo - intense pathologies were measured with respect to healthy white matter tissue from the same image. The inclusion criterion for images used for this study was at least one visible pathology. Reference Standard and Metrics: Linear Reconstruction was used as the reference standard for the comparison. Contrast -to-Noise Ratio (CNR) between pathology and healthy tissues was measured to quantify how accurately pathology features are preserved by Advanced Reconstruction. The mean CNR of Advanced Reconstruction was required to be greater than the mean CNR of the baseline Linear Reconstruction at statistical significance level of 0.05 for each sequence type. Dataset and Sample Size: A minimum of 16 images per model type (sequence group) were included for lesion annotation. Pathologies spanning multiple slices were used multiple times. All annotated images were then reviewed , and inaccurate ROI annotations were excluded from the analysis. The data meeting inclusion criteria are described below. Patients 15 Images 46 Pathologies 58 ROIs 464 Demographics and other Variability Gender Female 28% Male 70% Unknown 2% Age 35-60 28% 60+ 70% 2+* 2% Ethnicity Data Not Recorded Number of sites 2 Equipment Swoop v2 Pathology Atrophy, Demyelinating diseases, Embolic Infarct, Infarct, Intracereberal Hemorrhage, Intraparenchymal Hemorrhage, Lesion, Resection, Stroke, Thrombectomy, Tumor, White Matter Disease, White Matter Hyperintensity, White Matter Lesion *Partially anonymized Study Results: In all cases, CNR of', 'Advanced Reconstruction was greater than or equal to Linear Reconstruction for both hyper - and hypo -intense pathologies. The study result demonstrates that Advanced Reconstruction does not unexpectedly modify, remove, or reduce the con trast of pathology features. Advanced Reconstruction Image Validation Study Design: Five external, ABR -certified radiologists representing clinical users were asked to review side -by-side clinical image sets taken with the subject Swoop System, reconstructed with both Advanced and Linear Reconstruction. The reviewers rated the images usin g a five -point scale for image quality and the consistency of diagnosis using both methods in the categories of noise, sharpness, contrast, geometric fidelity, artifact, and overall image quality. Reference Standard and Metrics: Linear Reconstruction was used as the reference standard for the comparison. Advanced Reconstruction was required to perform at least as well as Linear Reconstruction in all categories (median score ≥0 on Likert scale) and perform better (≥1 on Likert scal e) in at least one of the quality -based categories. Dataset and Sample Size: A sample size of at least 16 was used per sequence. Within the sample dataset at least four cases for each sequence -available image orientation (axial, sagittal, coronal) were used. Patients 32 Images 167 Demographics and other Variability Gender Female 28% Male 70% Unknown 2% Age 18-35 1% 35-60 25% 60+ 35% 2+* 39% Ethnicity Data Not Recorded Number of sites 3 Equipment Swoop v2 Pathology Included pathology: Atrophy, Chronic Infarct, Demyelinating diseases, Embolic Infarct, Hemorrhage, Hydrocephalus, Infarct, Intracereberal Hemorrhage, Intraparenchymal Hemorrhage, Lesion, M1 Occlusion, Mass Effect, Stroke, Subdural Hemorrhage, Traumatic Brain Injury, Tumor, Wh ite Matter Disease *Partially anonymized Test Results: Advanced Reconstruction achieved a median score of 2 (the most positive rating scale value) in all categories. This scoring indicates reviewers found Advanced Reconstruction improved image quality while maintaining diagnostic', 'consistency relative to Linear Reconstruction. CONCLUSION Based on the intended use, technological characteristics, performance results, and comparison to the predicate, the subject Swoop Portable MR Imaging System (V2) has been shown to be substantially equivalent to the predicate and there are no new questions of safety and effectiveness.']\n",
            "{'Hardware': [], 'Software Components': ['width', 'package', 'the qs regulation', 'exam setup', 'a determination', 'this letter', 'further announcements', 'any federal statutes', 'your device', 'listing', 'global regulatory 351 new whitfield st guilford', 'either class ii', '(\"udi rule', 'your product', 'medical devices', 'more information', 'radiological imaging and radiation therapy devices oht8', 'the subject device', 'care', 'device software functions', 'unknown 10%', 'pathology', 'statistical significance level', 'internal requirements', 'the similarity', 'normalized mean squared error', '-to-noise', 'effectiveness', 'all annotated images', '(gray/white', 'industry', 'v2) image reconstruction', 'ed.4+a1', 'the test summaries', 'median score ≥0', 'safety', 'atrophy', 'outpatient', 'system', 'performance results', 'no new questions', 'technological characteristics', 'a question', 'model', 'artifacts', 'diagnosis', '1.6kg -200', 'noise', 'gender', 'the comparison'], 'AI Models': ['width', 'package', 'the qs regulation', 'exam setup', 'a determination', 'this letter', 'further announcements', 'any federal statutes', 'your device', 'listing', 'global regulatory 351 new whitfield st guilford', 'either class ii', '(\"udi rule', 'your product', 'medical devices', 'more information', 'radiological imaging and radiation therapy devices oht8', 'the subject device', 'care', 'device software functions', 'unknown 10%', 'pathology', 'statistical significance level', 'internal requirements', 'the similarity', 'normalized mean squared error', '-to-noise', 'effectiveness', 'all annotated images', '(gray/white', 'industry', 'v2) image reconstruction', 'ed.4+a1', 'the test summaries', 'median score ≥0', 'safety', 'atrophy', 'outpatient', 'system', 'performance results', 'no new questions', 'technological characteristics', 'a question', 'model', 'artifacts', 'diagnosis', '1.6kg -200', 'noise', 'gender', 'the comparison'], 'Data Pipelines': ['width', 'package', 'the qs regulation', 'exam setup', 'a determination', 'this letter', 'further announcements', 'any federal statutes', 'your device', 'listing', 'global regulatory 351 new whitfield st guilford', 'either class ii', '(\"udi rule', 'your product', 'medical devices', 'more information', 'radiological imaging and radiation therapy devices oht8', 'the subject device', 'care', 'device software functions', 'unknown 10%', 'pathology', 'statistical significance level', 'internal requirements', 'the similarity', 'normalized mean squared error', '-to-noise', 'effectiveness', 'all annotated images', '(gray/white', 'industry', 'v2) image reconstruction', 'ed.4+a1', 'the test summaries', 'median score ≥0', 'safety', 'atrophy', 'outpatient', 'system', 'performance results', 'no new questions', 'technological characteristics', 'a question', 'model', 'artifacts', 'diagnosis', '1.6kg -200', 'noise', 'gender', 'the comparison'], 'User Interface': ['width', 'package', 'the qs regulation', 'exam setup', 'a determination', 'this letter', 'further announcements', 'any federal statutes', 'your device', 'listing', 'global regulatory 351 new whitfield st guilford', 'either class ii', '(\"udi rule', 'your product', 'medical devices', 'more information', 'radiological imaging and radiation therapy devices oht8', 'the subject device', 'care', 'device software functions', 'unknown 10%', 'pathology', 'statistical significance level', 'internal requirements', 'the similarity', 'normalized mean squared error', '-to-noise', 'effectiveness', 'all annotated images', '(gray/white', 'industry', 'v2) image reconstruction', 'ed.4+a1', 'the test summaries', 'median score ≥0', 'safety', 'atrophy', 'outpatient', 'system', 'performance results', 'no new questions', 'technological characteristics', 'a question', 'model', 'artifacts', 'diagnosis', '1.6kg -200', 'noise', 'gender', 'the comparison'], 'Integration': ['width', 'package', 'the qs regulation', 'exam setup', 'a determination', 'this letter', 'further announcements', 'any federal statutes', 'your device', 'listing', 'global regulatory 351 new whitfield st guilford', 'either class ii', '(\"udi rule', 'your product', 'medical devices', 'more information', 'radiological imaging and radiation therapy devices oht8', 'the subject device', 'care', 'device software functions', 'unknown 10%', 'pathology', 'statistical significance level', 'internal requirements', 'the similarity', 'normalized mean squared error', '-to-noise', 'effectiveness', 'all annotated images', '(gray/white', 'industry', 'v2) image reconstruction', 'ed.4+a1', 'the test summaries', 'median score ≥0', 'safety', 'atrophy', 'outpatient', 'system', 'performance results', 'no new questions', 'technological characteristics', 'a question', 'model', 'artifacts', 'diagnosis', '1.6kg -200', 'noise', 'gender', 'the comparison']}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}