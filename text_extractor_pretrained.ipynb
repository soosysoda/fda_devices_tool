{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiCFGRO_HmcY",
        "outputId": "65eca4e3-4e6a-4f74-b142-32aa536eddcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.55.4)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Collecting keybert\n",
            "  Downloading keybert-0.9.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.16.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: rich>=10.4.0 in /usr/local/lib/python3.12/dist-packages (from keybert) (13.9.4)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.12/dist-packages (from keybert) (1.6.1)\n",
            "Requirement already satisfied: sentence-transformers>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from keybert) (5.1.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.8)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.4.0->keybert) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.4.0->keybert) (2.19.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->keybert) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->keybert) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.2->keybert) (3.6.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers>=0.3.8->keybert) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.4.0->keybert) (0.1.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Downloading keybert-0.9.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2, keybert\n",
            "Successfully installed PyPDF2-3.0.1 keybert-0.9.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m122.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers spacy keybert PyPDF2\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WHj4gnl8IJyl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import spacy\n",
        "from keybert import KeyBERT\n",
        "from PyPDF2 import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZQvevGitDny",
        "outputId": "fdd016c2-69a2-4ba8-c65d-88b084574d57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Running on GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "device = 0 if torch.cuda.is_available() else -1\n",
        "gpu_status = \"GPU: \" + torch.cuda.get_device_name(0) if device == 0 else \"CPU\"\n",
        "print(f\" Running on {gpu_status}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "kw_model = KeyBERT()"
      ],
      "metadata": {
        "id": "crCkwtQkkTN7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478,
          "referenced_widgets": [
            "d83fc1b45de34dd2b80ad8848d06a577",
            "760663f906ed4419b2c47d4a53a463d8",
            "8597e606c8cf468fa512e0156e2cd31b",
            "0334a4adea7b415fad9cbd78aa906e34",
            "aa34205dd2504bfcbbf67e4bc0eab429",
            "c61659be2506487da55dbfe04538f3fa",
            "ced30f24d2c74721b145c26aafad655d",
            "80ae8c8dd1d14351b70dd3ac27d6c095",
            "044cf43415ab4d8d9183962d1e32066b",
            "4f76dd4ecc464ad780f8696ca1a22add",
            "94857f3629a1403e8f76c5600f00c9a8",
            "725ab59bada74c0f91b36377a2bf9f3b",
            "fc051da87db6483eb3d3102f4d640367",
            "24559bbfa08d4015b3c1dc9707ab202c",
            "39b0fd26912d4d64aa251224fc5c0d74",
            "21e16c08b8f048cdaa92e53353e7d4c4",
            "6af9a125ffd54eb8bd5ea64ffc022a26",
            "78fb5c04676048da979c6c98d677bc69",
            "889fb11a9ab74f1682c3531c4f073abf",
            "e2bf9af041b84288947073bcdce45a4d",
            "5290d253b4ac4082b3844773cd2358c8",
            "15c9bbe5839f4667bbb287c3d6d09f2e",
            "25edc8b00e8b407d9ada9d6fed5385e6",
            "83c41b76b2d24fe5b72a5d6f3b7a8a85",
            "6394be944494423ea9a4b4fff2973767",
            "fa52558f26e04c259c68c3b1cae38a5f",
            "0aafbc2db2d74b29a0adc9ad15e9291a",
            "644a3c491b0b4a48a1e9096d82493a60",
            "4f2c09f40a794801bdb9348ec05ac498",
            "f748a66e7f0f4158852766f5522ceafe",
            "21d38fa0d4824d9ea7a6bfe52213056a",
            "1eb18ab3edd14621ab4949852c2d1d20",
            "a528990709d04c75a42a6e388aefb2ab",
            "8fa858a9ad4140259d2d45fff228d8e2",
            "f745a6e3a6274a48a95533b0102f46ec",
            "971f4081970047ca8634ee509311c1fa",
            "4745adf1201e40819f86bf183561eb98",
            "bf7eb8d35f434dcf83ce70a02ab9f218",
            "96750ef1eeaa4d1889528d144d632e5d",
            "e48d5cd7d9244f50a51e65913b359c59",
            "37e90907575746cebb44a09385eb355e",
            "468799b49e2e4b93aed3149d51707e25",
            "75ae6a37475c48f7b4468220045889a8",
            "f0d50225253148bf9c9d470bfb3c496a",
            "7c87d38bee5e4a3bb2b9ccad516c5ed2",
            "69c4b508304646f2bec8cfeb21a86077",
            "ea653737c9a3478d842eebe9175f9fbc",
            "5dd51a2c94d04dce8dd93dfab79da3af",
            "8fa386e9e1ab466aacf78a64e917abef",
            "622beac9125f4a759a292a744e17a61f",
            "90924b2b482b42c5b44f264dfaaf4aea",
            "ee672092ea114615a48aef0fb6d0be94",
            "23359c4419714e9f9c9a0f22f04a2463",
            "141ea1c2e9624cee9f94de2b6e2e6e22",
            "8463a1e3e4e84e3ba512f5e566fb85b3",
            "50965ee3f5a44c819e60fc90b61409d6",
            "ef3da60b638f4d7f9654e6b8544cb55d",
            "fa25cbad680745c28016d70e51fbfd03",
            "20a5ef2094d24275a822bb4b2f91c848",
            "c6aaef4a9d7c480bab2c35865d924bd0",
            "0f9664971b94425a89866558ff29c988",
            "2c0257a5a38943f58b8119b04355dc93",
            "99e335efa34e4c52aca371a76cf5012d",
            "600e279f228c4843a4a737fef3240c53",
            "9c715e9a35894837a442dcf3a347b141",
            "0f9bbc504b2a4de6a421f2e4e28d1700",
            "45cdeb04dec848bebfff87a1009bfa75",
            "abb5fb1f4e4a46c5989f7a7f873d33da",
            "b68e986d47b846a182c4ea8f5722835c",
            "ff3964788fab4416a66d242ca78d1511",
            "7f8aa8039b524fc199e1d2d2ddaf3237",
            "1c2d595a60f84a739946e6cb9511ecea",
            "4afa78d55d434a5d938546deef58b9ac",
            "d8faf331c916433eb5130d8f90335c36",
            "7e4d56c978424510aec17e018cd6075f",
            "452aae3a99ef49efa7072cda43204455",
            "3bc79833eefd456a9431643eddeb7599",
            "70fd871ad59c4dc1942ddcbfed7afe53",
            "dd3a4e14726b47cb89435805508757f0",
            "f47aa59a388e47659ca9e00b5b36e4a3",
            "7de223dd23a84c7f9f5fd44f5b773bd0",
            "5b74cd9fc0404be7bcc403e723e569fc",
            "cc9c46259c6845c0be375e8873d88e9c",
            "cc564cb975e7424d9ad697b5dfb9f0ad",
            "ec5e6aab8e994a2b93431ef17d6d04f4",
            "6f41baeafe0e4aaca797be2932d71c1d",
            "4c113b49bdc549b2bd25318fa90c760c",
            "4a960713eb204500b4af5481f6c2f2f0",
            "fb90f20d6d7f45149b438ebeb98dfce6",
            "8c53e0318e364603ab3c6bd906085317",
            "79484938dc444f6ea4f46154242154bb",
            "1068fa299b96472db7cf6b3aec40323e",
            "d1d07cf7f14d4d06a8d5b03acad41cf8",
            "35a5bb0540354c2c90f98ae8e4981774",
            "5979891941a04cd083ea61d15b475072",
            "2689711ca39845de96dcd255c78b36c1",
            "5491b01a02b541fd8370795b2e70b4d8",
            "3bade2bc6de34e79924f7d818682db78",
            "a97c12b308b84a6c847b4fa89097d53c",
            "3a4dce0466644296bf783ff42091458a",
            "7c901d76b1154f218ab8bdbc6f5d8bc8",
            "9104e994aaaa45bfb3ae4e6cfacc7f8a",
            "28105d6cd02345cbabd44dbc177ecf1f",
            "46f17f8ea4a34444ace4c07b9966fac4",
            "8b71a625157745f3a01df9421f813240",
            "6d88e43c2d5843f4b8c9c603ac456527",
            "e793e2b8e44b4037ba7684c79e0c8c03",
            "773341206b1e4dbab2158a8ecd5975ed",
            "5ac03e2313554fdc9ac089cb5fce1689",
            "7a8f93a1ef0d40188b30d6ed8051086c",
            "ee38fddc3ea04f4783e28b5abd46a2c9",
            "88530943af154cadbe1d44a0d46df019",
            "6b6a39ffd5be436c9f94770389a6eb7d",
            "b3f1e67a1ebe427c94902a4906514404",
            "2e1950989df74d2889bbbda0c272818a",
            "5c99467e28f340af984eb18a060173ea",
            "ea83d0d6a10545c8b832708f45ade510",
            "d5c0d19159824a3083bd84ef28ff5c59",
            "350aa079e1394e8788b9ef124c17d44c",
            "f1d506253f8a489c8f3afcb9b5ad2212",
            "40818bf45540497a8c273b314f45f12b"
          ]
        },
        "outputId": "02da3c50-b095-4113-b4bf-fbd7ccc582be"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d83fc1b45de34dd2b80ad8848d06a577"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "725ab59bada74c0f91b36377a2bf9f3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25edc8b00e8b407d9ada9d6fed5385e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8fa858a9ad4140259d2d45fff228d8e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c87d38bee5e4a3bb2b9ccad516c5ed2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50965ee3f5a44c819e60fc90b61409d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45cdeb04dec848bebfff87a1009bfa75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70fd871ad59c4dc1942ddcbfed7afe53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb90f20d6d7f45149b438ebeb98dfce6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3a4dce0466644296bf783ff42091458a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee38fddc3ea04f4783e28b5abd46a2c9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tiu2K2U2kkTi"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "goilOfXkqw3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c91bd73a-2dd3-4111-ab03-f4b94bf1f9af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/multitask_fda_extractor.zip\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/special_tokens_map.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/pytorch_model.bin? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/tokenizer_config.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/labels.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/tokenizer.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n"
          ]
        }
      ],
      "source": [
        "!unzip /content/drive/MyDrive/multitask_fda_extractor.zip -d /content/drive/MyDrive/multitask_fda_extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syFggDQHq0P-",
        "outputId": "7c9f6a70-e860-4bfa-e256-d03661edb700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/ and are newly initialized: ['bert.embeddings.LayerNorm.bias', 'bert.embeddings.LayerNorm.weight', 'bert.embeddings.position_embeddings.weight', 'bert.embeddings.token_type_embeddings.weight', 'bert.embeddings.word_embeddings.weight', 'bert.encoder.layer.0.attention.output.LayerNorm.bias', 'bert.encoder.layer.0.attention.output.LayerNorm.weight', 'bert.encoder.layer.0.attention.output.dense.bias', 'bert.encoder.layer.0.attention.output.dense.weight', 'bert.encoder.layer.0.attention.self.key.bias', 'bert.encoder.layer.0.attention.self.key.weight', 'bert.encoder.layer.0.attention.self.query.bias', 'bert.encoder.layer.0.attention.self.query.weight', 'bert.encoder.layer.0.attention.self.value.bias', 'bert.encoder.layer.0.attention.self.value.weight', 'bert.encoder.layer.0.intermediate.dense.bias', 'bert.encoder.layer.0.intermediate.dense.weight', 'bert.encoder.layer.0.output.LayerNorm.bias', 'bert.encoder.layer.0.output.LayerNorm.weight', 'bert.encoder.layer.0.output.dense.bias', 'bert.encoder.layer.0.output.dense.weight', 'bert.encoder.layer.1.attention.output.LayerNorm.bias', 'bert.encoder.layer.1.attention.output.LayerNorm.weight', 'bert.encoder.layer.1.attention.output.dense.bias', 'bert.encoder.layer.1.attention.output.dense.weight', 'bert.encoder.layer.1.attention.self.key.bias', 'bert.encoder.layer.1.attention.self.key.weight', 'bert.encoder.layer.1.attention.self.query.bias', 'bert.encoder.layer.1.attention.self.query.weight', 'bert.encoder.layer.1.attention.self.value.bias', 'bert.encoder.layer.1.attention.self.value.weight', 'bert.encoder.layer.1.intermediate.dense.bias', 'bert.encoder.layer.1.intermediate.dense.weight', 'bert.encoder.layer.1.output.LayerNorm.bias', 'bert.encoder.layer.1.output.LayerNorm.weight', 'bert.encoder.layer.1.output.dense.bias', 'bert.encoder.layer.1.output.dense.weight', 'bert.encoder.layer.10.attention.output.LayerNorm.bias', 'bert.encoder.layer.10.attention.output.LayerNorm.weight', 'bert.encoder.layer.10.attention.output.dense.bias', 'bert.encoder.layer.10.attention.output.dense.weight', 'bert.encoder.layer.10.attention.self.key.bias', 'bert.encoder.layer.10.attention.self.key.weight', 'bert.encoder.layer.10.attention.self.query.bias', 'bert.encoder.layer.10.attention.self.query.weight', 'bert.encoder.layer.10.attention.self.value.bias', 'bert.encoder.layer.10.attention.self.value.weight', 'bert.encoder.layer.10.intermediate.dense.bias', 'bert.encoder.layer.10.intermediate.dense.weight', 'bert.encoder.layer.10.output.LayerNorm.bias', 'bert.encoder.layer.10.output.LayerNorm.weight', 'bert.encoder.layer.10.output.dense.bias', 'bert.encoder.layer.10.output.dense.weight', 'bert.encoder.layer.11.attention.output.LayerNorm.bias', 'bert.encoder.layer.11.attention.output.LayerNorm.weight', 'bert.encoder.layer.11.attention.output.dense.bias', 'bert.encoder.layer.11.attention.output.dense.weight', 'bert.encoder.layer.11.attention.self.key.bias', 'bert.encoder.layer.11.attention.self.key.weight', 'bert.encoder.layer.11.attention.self.query.bias', 'bert.encoder.layer.11.attention.self.query.weight', 'bert.encoder.layer.11.attention.self.value.bias', 'bert.encoder.layer.11.attention.self.value.weight', 'bert.encoder.layer.11.intermediate.dense.bias', 'bert.encoder.layer.11.intermediate.dense.weight', 'bert.encoder.layer.11.output.LayerNorm.bias', 'bert.encoder.layer.11.output.LayerNorm.weight', 'bert.encoder.layer.11.output.dense.bias', 'bert.encoder.layer.11.output.dense.weight', 'bert.encoder.layer.2.attention.output.LayerNorm.bias', 'bert.encoder.layer.2.attention.output.LayerNorm.weight', 'bert.encoder.layer.2.attention.output.dense.bias', 'bert.encoder.layer.2.attention.output.dense.weight', 'bert.encoder.layer.2.attention.self.key.bias', 'bert.encoder.layer.2.attention.self.key.weight', 'bert.encoder.layer.2.attention.self.query.bias', 'bert.encoder.layer.2.attention.self.query.weight', 'bert.encoder.layer.2.attention.self.value.bias', 'bert.encoder.layer.2.attention.self.value.weight', 'bert.encoder.layer.2.intermediate.dense.bias', 'bert.encoder.layer.2.intermediate.dense.weight', 'bert.encoder.layer.2.output.LayerNorm.bias', 'bert.encoder.layer.2.output.LayerNorm.weight', 'bert.encoder.layer.2.output.dense.bias', 'bert.encoder.layer.2.output.dense.weight', 'bert.encoder.layer.3.attention.output.LayerNorm.bias', 'bert.encoder.layer.3.attention.output.LayerNorm.weight', 'bert.encoder.layer.3.attention.output.dense.bias', 'bert.encoder.layer.3.attention.output.dense.weight', 'bert.encoder.layer.3.attention.self.key.bias', 'bert.encoder.layer.3.attention.self.key.weight', 'bert.encoder.layer.3.attention.self.query.bias', 'bert.encoder.layer.3.attention.self.query.weight', 'bert.encoder.layer.3.attention.self.value.bias', 'bert.encoder.layer.3.attention.self.value.weight', 'bert.encoder.layer.3.intermediate.dense.bias', 'bert.encoder.layer.3.intermediate.dense.weight', 'bert.encoder.layer.3.output.LayerNorm.bias', 'bert.encoder.layer.3.output.LayerNorm.weight', 'bert.encoder.layer.3.output.dense.bias', 'bert.encoder.layer.3.output.dense.weight', 'bert.encoder.layer.4.attention.output.LayerNorm.bias', 'bert.encoder.layer.4.attention.output.LayerNorm.weight', 'bert.encoder.layer.4.attention.output.dense.bias', 'bert.encoder.layer.4.attention.output.dense.weight', 'bert.encoder.layer.4.attention.self.key.bias', 'bert.encoder.layer.4.attention.self.key.weight', 'bert.encoder.layer.4.attention.self.query.bias', 'bert.encoder.layer.4.attention.self.query.weight', 'bert.encoder.layer.4.attention.self.value.bias', 'bert.encoder.layer.4.attention.self.value.weight', 'bert.encoder.layer.4.intermediate.dense.bias', 'bert.encoder.layer.4.intermediate.dense.weight', 'bert.encoder.layer.4.output.LayerNorm.bias', 'bert.encoder.layer.4.output.LayerNorm.weight', 'bert.encoder.layer.4.output.dense.bias', 'bert.encoder.layer.4.output.dense.weight', 'bert.encoder.layer.5.attention.output.LayerNorm.bias', 'bert.encoder.layer.5.attention.output.LayerNorm.weight', 'bert.encoder.layer.5.attention.output.dense.bias', 'bert.encoder.layer.5.attention.output.dense.weight', 'bert.encoder.layer.5.attention.self.key.bias', 'bert.encoder.layer.5.attention.self.key.weight', 'bert.encoder.layer.5.attention.self.query.bias', 'bert.encoder.layer.5.attention.self.query.weight', 'bert.encoder.layer.5.attention.self.value.bias', 'bert.encoder.layer.5.attention.self.value.weight', 'bert.encoder.layer.5.intermediate.dense.bias', 'bert.encoder.layer.5.intermediate.dense.weight', 'bert.encoder.layer.5.output.LayerNorm.bias', 'bert.encoder.layer.5.output.LayerNorm.weight', 'bert.encoder.layer.5.output.dense.bias', 'bert.encoder.layer.5.output.dense.weight', 'bert.encoder.layer.6.attention.output.LayerNorm.bias', 'bert.encoder.layer.6.attention.output.LayerNorm.weight', 'bert.encoder.layer.6.attention.output.dense.bias', 'bert.encoder.layer.6.attention.output.dense.weight', 'bert.encoder.layer.6.attention.self.key.bias', 'bert.encoder.layer.6.attention.self.key.weight', 'bert.encoder.layer.6.attention.self.query.bias', 'bert.encoder.layer.6.attention.self.query.weight', 'bert.encoder.layer.6.attention.self.value.bias', 'bert.encoder.layer.6.attention.self.value.weight', 'bert.encoder.layer.6.intermediate.dense.bias', 'bert.encoder.layer.6.intermediate.dense.weight', 'bert.encoder.layer.6.output.LayerNorm.bias', 'bert.encoder.layer.6.output.LayerNorm.weight', 'bert.encoder.layer.6.output.dense.bias', 'bert.encoder.layer.6.output.dense.weight', 'bert.encoder.layer.7.attention.output.LayerNorm.bias', 'bert.encoder.layer.7.attention.output.LayerNorm.weight', 'bert.encoder.layer.7.attention.output.dense.bias', 'bert.encoder.layer.7.attention.output.dense.weight', 'bert.encoder.layer.7.attention.self.key.bias', 'bert.encoder.layer.7.attention.self.key.weight', 'bert.encoder.layer.7.attention.self.query.bias', 'bert.encoder.layer.7.attention.self.query.weight', 'bert.encoder.layer.7.attention.self.value.bias', 'bert.encoder.layer.7.attention.self.value.weight', 'bert.encoder.layer.7.intermediate.dense.bias', 'bert.encoder.layer.7.intermediate.dense.weight', 'bert.encoder.layer.7.output.LayerNorm.bias', 'bert.encoder.layer.7.output.LayerNorm.weight', 'bert.encoder.layer.7.output.dense.bias', 'bert.encoder.layer.7.output.dense.weight', 'bert.encoder.layer.8.attention.output.LayerNorm.bias', 'bert.encoder.layer.8.attention.output.LayerNorm.weight', 'bert.encoder.layer.8.attention.output.dense.bias', 'bert.encoder.layer.8.attention.output.dense.weight', 'bert.encoder.layer.8.attention.self.key.bias', 'bert.encoder.layer.8.attention.self.key.weight', 'bert.encoder.layer.8.attention.self.query.bias', 'bert.encoder.layer.8.attention.self.query.weight', 'bert.encoder.layer.8.attention.self.value.bias', 'bert.encoder.layer.8.attention.self.value.weight', 'bert.encoder.layer.8.intermediate.dense.bias', 'bert.encoder.layer.8.intermediate.dense.weight', 'bert.encoder.layer.8.output.LayerNorm.bias', 'bert.encoder.layer.8.output.LayerNorm.weight', 'bert.encoder.layer.8.output.dense.bias', 'bert.encoder.layer.8.output.dense.weight', 'bert.encoder.layer.9.attention.output.LayerNorm.bias', 'bert.encoder.layer.9.attention.output.LayerNorm.weight', 'bert.encoder.layer.9.attention.output.dense.bias', 'bert.encoder.layer.9.attention.output.dense.weight', 'bert.encoder.layer.9.attention.self.key.bias', 'bert.encoder.layer.9.attention.self.key.weight', 'bert.encoder.layer.9.attention.self.query.bias', 'bert.encoder.layer.9.attention.self.query.weight', 'bert.encoder.layer.9.attention.self.value.bias', 'bert.encoder.layer.9.attention.self.value.weight', 'bert.encoder.layer.9.intermediate.dense.bias', 'bert.encoder.layer.9.intermediate.dense.weight', 'bert.encoder.layer.9.output.LayerNorm.bias', 'bert.encoder.layer.9.output.LayerNorm.weight', 'bert.encoder.layer.9.output.dense.bias', 'bert.encoder.layer.9.output.dense.weight', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "MODEL_PATH = \"/content/drive/MyDrive/multitask_fda_extractor/content/multitask_fda_extractor/\"\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_PATH)\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_PATH, num_labels=6)  # 6 heads\n",
        "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "u8HCmMtvJWRI"
      },
      "outputs": [],
      "source": [
        "HEADS = [\"Hardware\", \"Software Components\", \"AI Models\", \"Data Pipelines\", \"User Interface\", \"Integration\"]\n",
        "# Helper: run multitask predictions\n",
        "def predict_heads(text):\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits.squeeze()\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()\n",
        "    return {HEADS[i]: float(probs[i]) for i in range(len(HEADS))}\n",
        "\n",
        "# Helper: extract keywords from text\n",
        "def extract_keywords(text, top_n=5):\n",
        "    doc = nlp(text)\n",
        "    chunks = list(set(chunk.text.lower() for chunk in doc.noun_chunks if len(chunk.text) > 3))\n",
        "    if chunks:\n",
        "        return chunks[:top_n]\n",
        "    keybert_kw = kw_model.extract_keywords(text, top_n=top_n)\n",
        "    return [kw for kw, _ in keybert_kw]\n",
        "\n",
        "# Split PDF into text chunks\n",
        "def extract_chunks_from_pdf(pdf_path, chunk_size=300):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    full_text = \" \".join([page.extract_text() for page in reader.pages if page.extract_text()])\n",
        "    words = full_text.split()\n",
        "    chunks = [\" \".join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    print(f\"Extracted {len(chunks)} chunks from PDF: {chunks}\")\n",
        "    return chunks\n",
        "\n",
        "# Main pipeline: classify chunks + extract keywords with scores\n",
        "def analyze_text_chunks(chunks, threshold=0.5):\n",
        "    results = {head: {} for head in HEADS} # Change to dictionary to store scores\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        head_probs = predict_heads(chunk)\n",
        "        for head, prob in head_probs.items():\n",
        "            if prob >= threshold:\n",
        "                kws = extract_keywords(chunk)\n",
        "                print(f\"Chunk {i+1}: Extracted keywords for '{head}' (score: {prob:.4f}): {kws}\")\n",
        "                for kw in kws:\n",
        "                    # Store keyword with its probability from the head prediction\n",
        "                    # We'll use the chunk's probability for simplicity here,\n",
        "                    # but ideally keyword relevance could also be considered.\n",
        "                    results[head][kw] = prob # Store the probability\n",
        "    # No need to remove duplicates and convert to list as dictionary handles uniqueness\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instead of uploading a PDF, take text input directly\n",
        "text_input = input(\"Enter text: \")\n",
        "\n",
        "# Wrap the text input in a list to match the expected input of analyze_text_chunks\n",
        "chunks = [text_input]\n",
        "\n",
        "# Analyze the text chunks\n",
        "output = analyze_text_chunks(chunks)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wyklux6Y-mTw",
        "outputId": "2c947d83-077d-411f-cb94-bb6ecb237d2d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter text: TAIMedImg DeepMets is a software application system intended for use in the contouring (segmentation) of brain magnetic resonance (MR) images. The device comprises an AI inference module and a DICOM Radiotherapy Structure Sets (RTSS, or RTSTRUCT) converter module. The AI inference module consists of image preprocessing, deep learning neural networks, and postprocessing components, and is intended to contour brain metastasis on the axial T1-weighted contrast-enhanced (T1WI+C) MR images. It utilizes deep learning neural networks to generate contours and annotations for the diagnosed brain metastases. The DICOM RTSS converter module converts the contours, annotations, along with metadata, into a standard DICOM-RTSTRUCT file, making it compatible with radiotherapy treatment planning systems.\n",
            "Chunk 1: Extracted keywords for 'AI Models' (score: 0.5676): ['(mr) images', 'the device', 'a dicom radiotherapy structure sets', 'the axial t1-weighted contrast-enhanced (t1wi+c) mr images', 'the ai inference module']\n",
            "Chunk 1: Extracted keywords for 'Data Pipelines' (score: 0.5966): ['(mr) images', 'the device', 'a dicom radiotherapy structure sets', 'the axial t1-weighted contrast-enhanced (t1wi+c) mr images', 'the ai inference module']\n",
            "Chunk 1: Extracted keywords for 'User Interface' (score: 0.5554): ['(mr) images', 'the device', 'a dicom radiotherapy structure sets', 'the axial t1-weighted contrast-enhanced (t1wi+c) mr images', 'the ai inference module']\n",
            "Chunk 1: Extracted keywords for 'Integration' (score: 0.5489): ['(mr) images', 'the device', 'a dicom radiotherapy structure sets', 'the axial t1-weighted contrast-enhanced (t1wi+c) mr images', 'the ai inference module']\n",
            "{'Hardware': {}, 'Software Components': {}, 'AI Models': {'(mr) images': 0.567646324634552, 'the device': 0.567646324634552, 'a dicom radiotherapy structure sets': 0.567646324634552, 'the axial t1-weighted contrast-enhanced (t1wi+c) mr images': 0.567646324634552, 'the ai inference module': 0.567646324634552}, 'Data Pipelines': {'(mr) images': 0.5965889096260071, 'the device': 0.5965889096260071, 'a dicom radiotherapy structure sets': 0.5965889096260071, 'the axial t1-weighted contrast-enhanced (t1wi+c) mr images': 0.5965889096260071, 'the ai inference module': 0.5965889096260071}, 'User Interface': {'(mr) images': 0.5554412603378296, 'the device': 0.5554412603378296, 'a dicom radiotherapy structure sets': 0.5554412603378296, 'the axial t1-weighted contrast-enhanced (t1wi+c) mr images': 0.5554412603378296, 'the ai inference module': 0.5554412603378296}, 'Integration': {'(mr) images': 0.5488832592964172, 'the device': 0.5488832592964172, 'a dicom radiotherapy structure sets': 0.5488832592964172, 'the axial t1-weighted contrast-enhanced (t1wi+c) mr images': 0.5488832592964172, 'the ai inference module': 0.5488832592964172}}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}